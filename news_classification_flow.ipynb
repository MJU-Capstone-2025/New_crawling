{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cW0rSYkO1gIT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Coffee News Price Direction Pipeline\n",
        "------------------------------------\n",
        "This script performs the full workflow for:\n",
        "1. Filtering coffee-related news articles\n",
        "2. Merging with coffee price data\n",
        "3. Generating manual and FinBERT-based labels\n",
        "4. Merging and feature engineering\n",
        "5. Modeling using classical ML (LogReg, RF, GBoost)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. 뉴스 필터링\n",
        "def filter_news_by_keywords(news_list, keywords):\n",
        "    filtered_dfs = []\n",
        "    for filename in news_list:\n",
        "        df = pd.read_csv(f'./data/{filename}')\n",
        "        df['is_price_related'] = df['title'].astype(str).apply(lambda x: any(k in x.lower() for k in keywords))\n",
        "        filtered_dfs.append(df[df['is_price_related']])\n",
        "    return pd.concat(filtered_dfs, ignore_index=True)\n",
        "\n",
        "# 2. 수동 라벨링\n",
        "def generate_manual_labels(news_df, price_df_path, output_path):\n",
        "    coffee_price_df = pd.read_csv(price_df_path)\n",
        "    news_df[\"date\"] = pd.to_datetime(news_df[\"date\"])\n",
        "    coffee_price_df[\"Date\"] = pd.to_datetime(coffee_price_df[\"Date\"])\n",
        "    coffee_price_df.sort_values(\"Date\", inplace=True)\n",
        "    coffee_price_df[\"prev_price\"] = coffee_price_df[\"Coffee_Price\"].shift(1)\n",
        "    coffee_price_df[\"price_direction\"] = coffee_price_df.apply(\n",
        "        lambda row: \"rise\" if row[\"Coffee_Price\"] > row[\"prev_price\"]\n",
        "        else \"fall\" if row[\"Coffee_Price\"] < row[\"prev_price\"]\n",
        "        else \"neutral\", axis=1)\n",
        "    merged_df = pd.merge(news_df, coffee_price_df[[\"Date\", \"price_direction\"]],\n",
        "                         left_on=\"date\", right_on=\"Date\", how=\"left\")\n",
        "    merged_df.rename(columns={\"price_direction\": \"manual_price_direction\"}, inplace=True)\n",
        "    merged_df.drop(columns=[\"Date\"], inplace=True)\n",
        "    merged_df.to_csv(output_path, index=False)\n",
        "    return merged_df\n",
        "\n",
        "# 3. FinBERT 추론\n",
        "def label_with_finbert(df, model_name=\"yiyanghkust/finbert-tone\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    titles = df[\"title\"].astype(str).tolist()\n",
        "    inputs = tokenizer(titles, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, dim=1).numpy()\n",
        "    id2label = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "    label_map = {\"positive\": \"rise\", \"negative\": \"fall\", \"neutral\": \"neutral\"}\n",
        "    df[\"price_direction\"] = [label_map[id2label[p]] for p in preds]\n",
        "    df.to_csv(\"./processed/finbert_coffee_news_labeled.csv\", index=False)\n",
        "    return df\n",
        "\n",
        "# 4. 데이터 통합 및 전처리\n",
        "def combine_and_engineer_features(finbert_df, manual_df):\n",
        "    merged = pd.merge(finbert_df, manual_df, on=[\"date\", \"title\", \"url\", \"is_price_related\"], how=\"left\", suffixes=(\"_finbert\", \"_title\"))\n",
        "    merged[\"combined_price_direction\"] = merged[\"price_direction\"].fillna(merged[\"manual_price_direction\"])\n",
        "    merged.dropna(subset=[\"combined_price_direction\"], inplace=True)\n",
        "\n",
        "    nltk.download(\"stopwords\")\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def preprocess(text):\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
        "        return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    merged[\"preprocessed_title\"] = merged[\"title\"].astype(str).apply(preprocess)\n",
        "    sentiments = merged[\"preprocessed_title\"].apply(analyzer.polarity_scores)\n",
        "    merged[\"positive_sentiment\"] = sentiments.apply(lambda x: x[\"pos\"])\n",
        "    merged[\"negative_sentiment\"] = sentiments.apply(lambda x: x[\"neg\"])\n",
        "    merged[\"neutral_sentiment\"] = sentiments.apply(lambda x: x[\"neu\"])\n",
        "\n",
        "    price_keywords = [\"rise\", \"increase\", \"jump\", \"surge\", \"climb\", \"fall\", \"drop\", \"decrease\", \"decline\", \"plunge\"]\n",
        "    for word in price_keywords:\n",
        "        merged[f\"{word}_present\"] = merged[\"preprocessed_title\"].str.contains(word).astype(int)\n",
        "\n",
        "    merged[\"weighted_positive_sentiment\"] = merged.apply(lambda x: x[\"positive_sentiment\"] * 2 if x[\"combined_price_direction\"] in [\"rise\", \"fall\"] else x[\"positive_sentiment\"], axis=1)\n",
        "    merged[\"weighted_negative_sentiment\"] = merged.apply(lambda x: x[\"negative_sentiment\"] * 2 if x[\"combined_price_direction\"] in [\"rise\", \"fall\"] else x[\"negative_sentiment\"], axis=1)\n",
        "    return merged\n",
        "\n",
        "# 5. 모델 학습 및 평가\n",
        "def train_classical_models(df):\n",
        "    features = ['preprocessed_title', 'positive_sentiment', 'negative_sentiment', 'neutral_sentiment'] +                [col for col in df.columns if '_present' in col] +                ['weighted_positive_sentiment', 'weighted_negative_sentiment']\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"combined_price_direction\"]\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(X[\"preprocessed_title\"])\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "    X = pd.concat([X.drop(\"preprocessed_title\", axis=1), tfidf_df], axis=1)\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y_resampled)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)\n",
        "        print(f\"Model: {name}\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(f\"F1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "        try:\n",
        "            print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob, multi_class='ovr'):.4f}\")\n",
        "        except:\n",
        "            print(\"AUC-ROC could not be calculated\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print()\n",
        "\n",
        "    joblib.dump(model, \"./models/gradient_boosting_model.pkl\")\n",
        "    joblib.dump(tfidf, \"./models/tfidf_vectorizer.pkl\")\n",
        "    joblib.dump(le, \"./models/label_encoder.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 뉴스 필터링\n",
        "news_list = [\n",
        "'dailycoffeenews_250331.csv',\n",
        "'usda_coffee_articles_250520.csv',\n",
        "'usda_robusta_articles_250520.csv',\n",
        "'worldcoffeeportal_250331.csv'\n",
        "]\n",
        "price_keywords = [\n",
        "\"coffee\", \"arabica\", \"robusta\", \"futures\", \"coffee price\", \"commodity\",\n",
        "\"coffee market\", \"crop\", \"harvest\", \"supply\", \"demand\", \"weather\",\n",
        "\"drought\", \"frost\", \"yield\", \"shortage\", \"production\", \"export\", \"import\",\n",
        "\"inventory\", \"logistics\", \"shipping\", \"rain\", \"climate\"\n",
        "]\n",
        "filtered_df = filter_news_by_keywords(news_list, price_keywords)\n",
        "filtered_df.to_csv(\"./processed/filtered_price_related_news.csv\", index=False)\n",
        "print(\"Step 1 완료: 필터링된 뉴스 저장\")\n",
        "\n",
        "# 2. 수동 라벨링\n",
        "manual_labeled_df = generate_manual_labels(filtered_df, \"./data/coffee_c_price.csv\", \"./processed/full_title_based_classification.csv\")\n",
        "print(\"Step 2 완료: 수동 라벨링 저장\")\n",
        "\n",
        "# 3. FinBERT 라벨링\n",
        "finbert_labeled_df = label_with_finbert(filtered_df)\n",
        "print(\"Step 3 완료: FinBERT 라벨링 저장\")\n",
        "\n",
        "# 4. 통합 및 전처리\n",
        "final_df = combine_and_engineer_features(finbert_labeled_df, manual_labeled_df)\n",
        "print(\"Step 4 완료: 데이터 병합 및 피처 생성\")\n",
        "\n",
        "# 5. 모델 학습 및 평가\n",
        "train_classical_models(final_df)\n",
        "print(\"Step 5 완료: 모델 학습 및 평가 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-L6F-w41iXF",
        "outputId": "19a9abc1-5d3f-4d6a-ead6-544e7ed8e547"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 완료: 필터링된 뉴스 저장\n",
            "Step 2 완료: 수동 라벨링 저장\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 완료: FinBERT 라벨링 저장\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4 완료: 데이터 병합 및 피처 생성\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.9928\n",
            "F1-score: 0.9928\n",
            "AUC-ROC: 1.0000\n",
            "[[787   2   0]\n",
            " [  3 787   0]\n",
            " [  2  10 778]]\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.9911\n",
            "F1-score: 0.9911\n",
            "AUC-ROC: 0.9995\n",
            "[[784   4   1]\n",
            " [  7 782   1]\n",
            " [  0   8 782]]\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.9287\n",
            "F1-score: 0.9286\n",
            "AUC-ROC: 0.9883\n",
            "[[720  36  33]\n",
            " [ 13 769   8]\n",
            " [ 19  60 711]]\n",
            "\n",
            "Step 5 완료: 모델 학습 및 평가 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_data(news_df, model_path, vectorizer_path, label_encoder_path):\n",
        "    import joblib\n",
        "    import re\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    from nltk.corpus import stopwords\n",
        "    import nltk\n",
        "\n",
        "    # Load trained components\n",
        "    model = joblib.load(model_path)\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "    label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "    # Preprocess\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "    nltk.download(\"stopwords\")\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def preprocess(text):\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
        "        return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    news_df[\"preprocessed_title\"] = news_df[\"title\"].astype(str).apply(preprocess)\n",
        "    sentiments = news_df[\"preprocessed_title\"].apply(analyzer.polarity_scores)\n",
        "    news_df[\"positive_sentiment\"] = sentiments.apply(lambda x: x[\"pos\"])\n",
        "    news_df[\"negative_sentiment\"] = sentiments.apply(lambda x: x[\"neg\"])\n",
        "    news_df[\"neutral_sentiment\"] = sentiments.apply(lambda x: x[\"neu\"])\n",
        "\n",
        "    price_keywords = [\"rise\", \"increase\", \"jump\", \"surge\", \"climb\", \"fall\", \"drop\", \"decrease\", \"decline\", \"plunge\"]\n",
        "    for word in price_keywords:\n",
        "        news_df[f\"{word}_present\"] = news_df[\"preprocessed_title\"].str.contains(word).astype(int)\n",
        "\n",
        "    news_df[\"weighted_positive_sentiment\"] = news_df[\"positive_sentiment\"]\n",
        "    news_df[\"weighted_negative_sentiment\"] = news_df[\"negative_sentiment\"]\n",
        "\n",
        "    # TF-IDF vectorize\n",
        "    tfidf = vectorizer.transform(news_df[\"preprocessed_title\"])\n",
        "    tfidf_df = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=news_df.index)\n",
        "\n",
        "    # Feature assembly\n",
        "    basic_features = ['positive_sentiment', 'negative_sentiment', 'neutral_sentiment'] + \\\n",
        "                     [f\"{w}_present\" for w in price_keywords] + \\\n",
        "                     ['weighted_positive_sentiment', 'weighted_negative_sentiment']\n",
        "\n",
        "    X = pd.concat([news_df[basic_features], tfidf_df], axis=1)\n",
        "\n",
        "    # Predict\n",
        "    y_pred_encoded = model.predict(X)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "    news_df[\"predicted_price_direction\"] = y_pred\n",
        "\n",
        "    return news_df\n"
      ],
      "metadata": {
        "id": "LKeOug1m2fm8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 뉴스 데이터 불러오기\n",
        "new_df = pd.read_csv(\"./data/new_news_250601.csv\")\n",
        "filtered_df = filter_news_by_keywords([\"new_news_250601.csv\"], price_keywords)\n",
        "\n",
        "# 예측 수행\n",
        "result_df = predict_new_data(\n",
        "    filtered_df,\n",
        "    model_path=\"./models/gradient_boosting_model.pkl\",\n",
        "    vectorizer_path=\"./models/tfidf_vectorizer.pkl\",\n",
        "    label_encoder_path=\"./models/label_encoder.pkl\"\n",
        ")\n",
        "\n",
        "# 결과 확인\n",
        "print(result_df[[\"title\", \"predicted_price_direction\"]].head())\n",
        "\n",
        "# 저장\n",
        "result_df.to_csv(\"./predictions/predicted_news_250601.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr1KOeC27XVG",
        "outputId": "1579bc01-6c5f-4ce6-8fd2-860eed484a19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title predicted_price_direction\n",
            "0          USDA releases 2025 crop production report                   neutral\n",
            "1  Volatility in futures market drives coffee unc...                   neutral\n",
            "2  Climate change threatens coffee production lon...                      fall\n",
            "3            Shipping delays impact coffee importers                   neutral\n",
            "4  New USDA undersecretary discusses coffee trade...                   neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}