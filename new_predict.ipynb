{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Coffee News Price Direction Pipeline\n",
        "------------------------------------\n",
        "This script performs the full workflow for:\n",
        "1. Filtering coffee-related news articles\n",
        "2. Merging with coffee price data\n",
        "3. Generating manual and FinBERT-based labels\n",
        "4. Merging and feature engineering\n",
        "5. Modeling using classical ML (LogReg, RF, GBoost)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. 뉴스 필터링\n",
        "def filter_news_by_keywords(news_list, keywords):\n",
        "    filtered_dfs = []\n",
        "    for filename in news_list:\n",
        "        df = pd.read_csv(f'./data/{filename}')\n",
        "        df['is_price_related'] = df['title'].astype(str).apply(lambda x: any(k in x.lower() for k in keywords))\n",
        "        filtered_dfs.append(df[df['is_price_related']])\n",
        "    return pd.concat(filtered_dfs, ignore_index=True)\n",
        "\n",
        "# 2. 수동 라벨링\n",
        "def generate_manual_labels(news_df, price_df_path, output_path):\n",
        "    coffee_price_df = pd.read_csv(price_df_path)\n",
        "    news_df[\"date\"] = pd.to_datetime(news_df[\"date\"])\n",
        "    coffee_price_df[\"Date\"] = pd.to_datetime(coffee_price_df[\"Date\"])\n",
        "    coffee_price_df.sort_values(\"Date\", inplace=True)\n",
        "    coffee_price_df[\"prev_price\"] = coffee_price_df[\"Coffee_Price\"].shift(1)\n",
        "    coffee_price_df[\"price_direction\"] = coffee_price_df.apply(\n",
        "        lambda row: \"rise\" if row[\"Coffee_Price\"] > row[\"prev_price\"]\n",
        "        else \"fall\" if row[\"Coffee_Price\"] < row[\"prev_price\"]\n",
        "        else \"neutral\", axis=1)\n",
        "    merged_df = pd.merge(news_df, coffee_price_df[[\"Date\", \"price_direction\"]],\n",
        "                         left_on=\"date\", right_on=\"Date\", how=\"left\")\n",
        "    merged_df.rename(columns={\"price_direction\": \"manual_price_direction\"}, inplace=True)\n",
        "    merged_df.drop(columns=[\"Date\"], inplace=True)\n",
        "    merged_df.to_csv(output_path, index=False)\n",
        "    return merged_df\n",
        "\n",
        "# 3. FinBERT 추론\n",
        "def label_with_finbert(df, model_name=\"yiyanghkust/finbert-tone\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    titles = df[\"title\"].astype(str).tolist()\n",
        "    inputs = tokenizer(titles, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, dim=1).numpy()\n",
        "    id2label = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "    label_map = {\"positive\": \"rise\", \"negative\": \"fall\", \"neutral\": \"neutral\"}\n",
        "    df[\"price_direction\"] = [label_map[id2label[p]] for p in preds]\n",
        "    df.to_csv(\"./processed/finbert_coffee_news_labeled.csv\", index=False)\n",
        "    return df\n",
        "\n",
        "# 4. 데이터 통합 및 전처리\n",
        "def combine_and_engineer_features(finbert_df, manual_df):\n",
        "    merged = pd.merge(finbert_df, manual_df, on=[\"date\", \"title\", \"url\", \"is_price_related\"], how=\"left\", suffixes=(\"_finbert\", \"_title\"))\n",
        "    merged[\"combined_price_direction\"] = merged[\"price_direction\"].fillna(merged[\"manual_price_direction\"])\n",
        "    merged.dropna(subset=[\"combined_price_direction\"], inplace=True)\n",
        "\n",
        "    nltk.download(\"stopwords\")\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def preprocess(text):\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
        "        return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    merged[\"preprocessed_title\"] = merged[\"title\"].astype(str).apply(preprocess)\n",
        "    sentiments = merged[\"preprocessed_title\"].apply(analyzer.polarity_scores)\n",
        "    merged[\"positive_sentiment\"] = sentiments.apply(lambda x: x[\"pos\"])\n",
        "    merged[\"negative_sentiment\"] = sentiments.apply(lambda x: x[\"neg\"])\n",
        "    merged[\"neutral_sentiment\"] = sentiments.apply(lambda x: x[\"neu\"])\n",
        "\n",
        "    price_keywords = [\"rise\", \"increase\", \"jump\", \"surge\", \"climb\", \"fall\", \"drop\", \"decrease\", \"decline\", \"plunge\"]\n",
        "    for word in price_keywords:\n",
        "        merged[f\"{word}_present\"] = merged[\"preprocessed_title\"].str.contains(word).astype(int)\n",
        "\n",
        "    merged[\"weighted_positive_sentiment\"] = merged.apply(lambda x: x[\"positive_sentiment\"] * 2 if x[\"combined_price_direction\"] in [\"rise\", \"fall\"] else x[\"positive_sentiment\"], axis=1)\n",
        "    merged[\"weighted_negative_sentiment\"] = merged.apply(lambda x: x[\"negative_sentiment\"] * 2 if x[\"combined_price_direction\"] in [\"rise\", \"fall\"] else x[\"negative_sentiment\"], axis=1)\n",
        "    return merged\n",
        "\n",
        "# 5. 모델 학습 및 평가\n",
        "def train_classical_models(df):\n",
        "    features = ['preprocessed_title', 'positive_sentiment', 'negative_sentiment', 'neutral_sentiment'] +                [col for col in df.columns if '_present' in col] +                ['weighted_positive_sentiment', 'weighted_negative_sentiment']\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[\"combined_price_direction\"]\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(X[\"preprocessed_title\"])\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "    X = pd.concat([X.drop(\"preprocessed_title\", axis=1), tfidf_df], axis=1)\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y_resampled)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "    }\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)\n",
        "        print(f\"Model: {name}\")\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "        print(f\"F1-score: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
        "        try:\n",
        "            print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob, multi_class='ovr'):.4f}\")\n",
        "        except:\n",
        "            print(\"AUC-ROC could not be calculated\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print()\n",
        "\n",
        "    joblib.dump(model, \"./models/gradient_boosting_model.pkl\")\n",
        "    joblib.dump(tfidf, \"./models/tfidf_vectorizer.pkl\")\n",
        "    joblib.dump(le, \"./models/label_encoder.pkl\")\n"
      ],
      "metadata": {
        "id": "8zR06jtuUMuu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_data(news_df, model_path, vectorizer_path, label_encoder_path):\n",
        "    import joblib\n",
        "    import re\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    from nltk.corpus import stopwords\n",
        "    import nltk\n",
        "\n",
        "    # Load trained components\n",
        "    model = joblib.load(model_path)\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "    label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "    # Preprocess\n",
        "    nltk.download(\"vader_lexicon\")\n",
        "    nltk.download(\"stopwords\")\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def preprocess(text):\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
        "        return \" \".join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    news_df[\"preprocessed_title\"] = news_df[\"title\"].astype(str).apply(preprocess)\n",
        "    sentiments = news_df[\"preprocessed_title\"].apply(analyzer.polarity_scores)\n",
        "    news_df[\"positive_sentiment\"] = sentiments.apply(lambda x: x[\"pos\"])\n",
        "    news_df[\"negative_sentiment\"] = sentiments.apply(lambda x: x[\"neg\"])\n",
        "    news_df[\"neutral_sentiment\"] = sentiments.apply(lambda x: x[\"neu\"])\n",
        "\n",
        "    price_keywords = [\"rise\", \"increase\", \"jump\", \"surge\", \"climb\", \"fall\", \"drop\", \"decrease\", \"decline\", \"plunge\"]\n",
        "    for word in price_keywords:\n",
        "        news_df[f\"{word}_present\"] = news_df[\"preprocessed_title\"].str.contains(word).astype(int)\n",
        "\n",
        "    news_df[\"weighted_positive_sentiment\"] = news_df[\"positive_sentiment\"]\n",
        "    news_df[\"weighted_negative_sentiment\"] = news_df[\"negative_sentiment\"]\n",
        "\n",
        "    # TF-IDF vectorize\n",
        "    tfidf = vectorizer.transform(news_df[\"preprocessed_title\"])\n",
        "    tfidf_df = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=news_df.index)\n",
        "\n",
        "    # Feature assembly\n",
        "    basic_features = ['positive_sentiment', 'negative_sentiment', 'neutral_sentiment'] + \\\n",
        "                     [f\"{w}_present\" for w in price_keywords] + \\\n",
        "                     ['weighted_positive_sentiment', 'weighted_negative_sentiment']\n",
        "\n",
        "    X = pd.concat([news_df[basic_features], tfidf_df], axis=1)\n",
        "\n",
        "    # Predict\n",
        "    y_pred_encoded = model.predict(X)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
        "    news_df[\"predicted_price_direction\"] = y_pred\n",
        "\n",
        "    return news_df\n"
      ],
      "metadata": {
        "id": "XzqtCRpzUP6p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBND1prKToAK",
        "outputId": "8b64bf70-8606-4350-cad7-7f87a5bbb805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title predicted_price_direction\n",
            "0  For Five Coffee Roasters Opens a Big One For A...                   neutral\n",
            "1  Xanadu Coffee Closed ‘Until Further Notice’ as...                   neutral\n",
            "2  Weekly Coffee News: Caffeine and Tinnitus + Re...                   neutral\n",
            "3  Hutsul Coffee Factory Carries Ukrainian Qualit...                   neutral\n",
            "4  Study Explores the ‘Arabica-Like’ Cup Qualitie...                   neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title predicted_price_direction\n",
            "0                       NASS 2025-01-24 15:00 Coffee                   neutral\n",
            "1  FAS 2024-12-18 15:00 Coffee: World Markets and...                   neutral\n",
            "2  Secretary Vilsack Highlights Historic Investme...                   neutral\n",
            "3  USDA Highlights Success of Historic Partnershi...                   neutral\n",
            "4  Coffee Is Hot Commodity, Even During This Reco...                      fall\n",
            "                                               title predicted_price_direction\n",
            "0  Secretary Rollins Suspends Live Animal Imports...                   neutral\n",
            "1  Fact Sheet: Biden-Harris Administration Advanc...                   neutral\n",
            "2  USDA Announces Grants and Technical Assistance...                   neutral\n",
            "3  Biden-Harris Administration Announces $90 Mill...                   neutral\n",
            "4  USDA Publishes Request for Information on the ...                   neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title predicted_price_direction\n",
            "0  Black Rifle Coffee Company appoints new Chief ...                   neutral\n",
            "1  Costa Coffee appoints new exec to lead US trav...                   neutral\n",
            "2  Celebrating 10 years of The Amsterdam Coffee F...                   neutral\n",
            "3  Diversified store network yields strong profit...                   neutral\n",
            "4  Jollibee Foods Corp outlines long-term coffee ...                   neutral\n"
          ]
        }
      ],
      "source": [
        "news_list = [\n",
        "'dailycoffeenews_250331.csv',\n",
        "'usda_coffee_articles_250520.csv',\n",
        "'usda_robusta_articles_250520.csv',\n",
        "'worldcoffeeportal_250331.csv'\n",
        "]\n",
        "\n",
        "price_keywords = [\n",
        "\"coffee\", \"arabica\", \"robusta\", \"futures\", \"coffee price\", \"commodity\",\n",
        "\"coffee market\", \"crop\", \"harvest\", \"supply\", \"demand\", \"weather\",\n",
        "\"drought\", \"frost\", \"yield\", \"shortage\", \"production\", \"export\", \"import\",\n",
        "\"inventory\", \"logistics\", \"shipping\", \"rain\", \"climate\"\n",
        "]\n",
        "\n",
        "for data in news_list:\n",
        "    filtered_df = filter_news_by_keywords([data], price_keywords)\n",
        "    filtered_df.to_csv(f\"./processed/filtered_{data}\", index=False)\n",
        "\n",
        "    result_df = predict_new_data(\n",
        "        filtered_df,\n",
        "        model_path=\"./models/gradient_boosting_model.pkl\",\n",
        "        vectorizer_path=\"./models/tfidf_vectorizer.pkl\",\n",
        "        label_encoder_path=\"./models/label_encoder.pkl\"\n",
        "    )\n",
        "\n",
        "    # 결과 확인\n",
        "    print(result_df[[\"title\", \"predicted_price_direction\"]].head())\n",
        "\n",
        "    # 저장\n",
        "    result_df.to_csv(f\"./predictions/predict_{data}\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-kms7zAUSK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}